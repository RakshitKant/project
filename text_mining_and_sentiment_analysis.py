# -*- coding: utf-8 -*-
"""Text Mining and Sentiment Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cVZk4loI06Oz87ywRP-bTpYiSK7anHbg
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
path='/content/drive/My Drive/dataset/Clothing Reviews.csv'
df=pd.read_csv(path)

df.head()

df.columns

df.drop(["Clothing ID", "Title"], axis=1, inplace=True)
df.head()

df.isnull().sum()

len(df)

df.describe()

df['Age'].unique()

df['Class Name'].unique()

df['Recommended IND'].unique()

df['Division Name'].unique()

df['Division Name'].unique()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

"""# Sentiment Analysis"""

import string
string.punctuation

df.dtypes

df['Review Text'].head()

df['Review Text'].dtypes

"""Removing the rows that has null values"""

df = df[~df['Review Text'].isnull()]

df['length']=df['Review Text'].apply(len)

df.head()

"""Length after removing the rows where Review Text is null"""

len(df)

"""**Length of the messages based on Recommended IND**"""

plt.figure(figsize=(10,8))
sns.boxplot(x='Recommended IND',y='length',data=df,hue='Rating',palette='rainbow')
plt.title('message length')

df.length.describe()

"""maximum length is 508.

**Maximum length review**
"""

df[df['length'] == 508]['Review Text'].iloc[0]

"""**Removing puntuations**"""

import string
string.punctuation
def remove_punctuation(text):
    no_punct=[words for words in text if words not in string.punctuation]
    words_wo_punct=''.join(no_punct)
    return words_wo_punct
df['Review Text']=df['Review Text'].apply(lambda x: remove_punctuation(x))
df.head()

"""**Text blob for polarity**

Using polarity we can classifity the reviews with as Positive, Negative or Neutral.
"""

from textblob import TextBlob
df.head()

df["Polarity"] = df["Review Text"].apply(lambda x: TextBlob(x).sentiment.polarity)

df.head()

df['Polarity'].min()

df['Polarity'].mean()

import plotly.express as px

px.histogram(df, x='Polarity', opacity = 0.5)

"""**Plot based on rating**"""

px.histogram(df, x='Polarity',color="Rating", opacity = 0.5)

def getAnalysis(polarity):
  if polarity < 0:
    return 'Negative'
  elif polarity == 0:
    return 'Neutral'
  else:
    return 'Positive'

df['TextBlob_Analysis']=df['Polarity'].apply(getAnalysis)

df.head()

polarity_plot=df['TextBlob_Analysis'].value_counts()

polarity_plot

"""**Plot for polarity based on Recommended IND**"""

sns.countplot(data=df, x='TextBlob_Analysis', hue='Recommended IND', palette='rainbow')

"""The product recommeded are given a value '1' and they are in positive polarity.

**Pie Chart for polarity**
"""

positive= 21213/len(df)*100
negative= 1333/len(df)*100
neutral= 95/len(df)*100

labels = ['Positive', 'Negative', 'Neutral']
sizes = [positive, negative, neutral]
plt.figure(figsize=(12,7))
plt.pie(sizes, labels=labels,autopct='%1.1f%%',shadow=True, startangle=140)
plt.show()

"""***Bar plot for polarity***"""

plt.figure(figsize=(10,8))
sns.barplot(polarity_plot.index, polarity_plot.values)

"""***Label Encoding***"""

#Import label encoder
from sklearn import preprocessing

# label_encoder object knows how to understand word labels.
label_encoder = preprocessing.LabelEncoder()

# Encode labels in column 'species'.
df['Encoded_value']= label_encoder.fit_transform(df['TextBlob_Analysis'])

df['Encoded_value'].unique()

df.head(5)

df[df['TextBlob_Analysis']=='Negative']['Encoded_value'].head()

"""Negative reviews are given 0 as encoded value, Positive as 2 and Neutral as 1"""

sns.boxplot(x="Rating", y="length", data=df,)

"""From the boxplot below,neutral Reviews(encoded value 0) length are smaller than the positive and negative reviews."""

sns.boxplot(x="Encoded_value", y="length", data=df,)

"""we can infer that, negative reviews( encoded value 0) are of a rating around 3, whereas the neutral reviews are with a rating around 3.5 and positive reviews are around 4.2

# ***Word Cloud***

we are splitting the dataframe based on the negative and positive reviews. To generate a word cloud of word which has a positive impact, we are creating a 'for loop' that iterates in the review column.

***Splitting into two data frames***
"""

df_positive = df[df["TextBlob_Analysis"] == "Positive"]
df_negative = df[df["TextBlob_Analysis"] == "Negative"]

df_negative['Polarity']=df_negative['Polarity'].astype(float)
df_positive['Polarity']=df_positive['Polarity'].astype(float)

df_positive.head()

from wordcloud import WordCloud, STOPWORDS

comment_words = ''
stopwords = set(STOPWORDS)

# iterate through the csv file
for val in df_positive["Review Text"]:

    # typecaste each val to string
    val = str(val)

    # split the value
    tokens = val.split()

    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 400, height = 400,
                background_color ='pink',max_words=100,
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

#save image of word cloud
#wordcloud.to_file("img/first_review.png")

"""***Finding top positive words and bigrams***"""

df['Polarity']=df['Polarity'].astype(float)

"""# **Finding frequent positive and negative words**"""

df_negative.head()

import numpy as np
import nltk
from nltk import word_tokenize
from nltk import ngrams
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
nltk.download("stopwords")
stop_words = set(stopwords.words('english'))
import warnings
warnings.filterwarnings("ignore")

df['Review Text'] = df['Review Text'].astype(str)

def text_process(mess):
  nopunc = [char for char in mess if char not in string.punctuation]
  nopunc = ''.join(nopunc)
  words= [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
  return words

df_positive['Review Text'].apply(text_process)
df_positive['Review_text_cleaned']=df['Review Text'].apply(text_process)

df_positive.head()

df_positive['Review_text_cleaned']=df_positive['Review_text_cleaned'].astype(str)

import heapq
from operator import itemgetter
from collections import Counter

def Top50(data,title=None):
    token_data= []
    tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
    for i in data:
        l = tokenizer.tokenize(i)
        token_data.append(l)
    corpus = []
    for i in token_data:
       for j in i:
         corpus.append(j)
    c = Counter(corpus)
    Di = dict(c)
    Top_50 = dict(heapq.nlargest(50, Di.items(), key=itemgetter(1)))
    dd = pd.DataFrame(Top_50.items(),columns=["word","frequency"])
    fig = plt.figure(1, figsize=(15, 15))
    plt.bar(range(len(Top_50)),Top_50.values(),align='center')
    plt.xticks(range(len(Top_50)), list(Top_50.keys()))
    plt.tick_params(axis="x",rotation=90)
    plt.tight_layout(pad = 0)
    if title==None:
        plt.title("Top 50 words")
    else:
        plt.title(title)
    return dd.head(10)

"""***Frequently used positive words***"""

Top50(df_positive["Review_text_cleaned"],title="Top 50 Positive words")
top_positive_words=Top50(df_positive["Review_text_cleaned"],title="Top 50 Positive words")
top_positive_words

"""***Frequent negative words***"""

df_negative['Review Text'].apply(text_process)
df_negative['Review_text_cleaned']=df_negative['Review Text'].apply(text_process)

df_negative['Review_text_cleaned']=df_negative['Review_text_cleaned'].astype(str)

Top50(df_negative["Review_text_cleaned"],title="Top 50 Negative words")
top_negative_words=Top50(df_negative["Review_text_cleaned"],title="Top 50 negative words")
top_negative_words

def text_preprocessing(words):
    tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
    nopunc = clean_text(text)
    tokenized_text = tokenizer.tokenize(nopunc)
    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]
    combined_text = ' '.join(remove_stopwords)
    return combined_text

"""***Bi-grams***"""

df['Review Text'].apply(text_process)
df['Review_text_cleaned']=df['Review Text'].apply(text_process)

df.head()

df['Review_text_cleaned']=df['Review_text_cleaned'].astype(str)

def Ngram(data,num,title=None):
    token_data= []
    tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
    for i in data:
        n_grams = ngrams(tokenizer.tokenize(i), num)
        p = [ ' '.join(grams) for grams in  n_grams]
        token_data.append(p)
    corpus = []
    for i in token_data:
       for j in i:
         corpus.append(j)
    c = Counter(corpus)
    Di = dict(c)
    Top_50 = dict(heapq.nlargest(50, Di.items(), key=itemgetter(1)))
    dd = pd.DataFrame(Top_50.items(),columns=["word","frequency"])
    fig = plt.figure(1, figsize=(15, 15))
    plt.bar(range(len(Top_50)),Top_50.values(),align='center')
    plt.xticks(range(len(Top_50)), list(Top_50.keys()))
    plt.tick_params(axis="x",rotation=90)
    plt.tight_layout(pad = 0)
    if title==None:
        plt.title("Ngram")
    else:
        plt.title(title)
    return dd.head(10)

Ngram(df["Review_text_cleaned"],2,title="Bigram of Reviews")

"""***Bigram of positive review***"""

Ngram(df_positive["Review_text_cleaned"],2,title="Bigram of Positive Reviews")

"""***Bi-grams for Negative Reviewa***"""

Ngram(df_negative["Review_text_cleaned"],2,title="Bigram of Negative Reviews")

"""# **correlation heatmap**"""

df.head()

df=df.drop(['Unnamed: 0','Positive Feedback Count'], axis=1)

df.head()

df.groupby('Encoded_value').count()

df.groupby('Encoded_value').mean()

"""***Heatmap***"""

plt.figure(figsize=(8,6))
sns.heatmap(df.groupby('Encoded_value').mean().corr(), annot=True)

"""***Cluster map***"""

plt.figure(figsize=(8,6))
sns.clustermap(df.groupby('Encoded_value').mean().corr(), annot=True, cmap='viridis')